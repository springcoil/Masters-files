\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amssymb, amsmath, textcomp}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{algorithm}[theorem]{Algorithm}
\newcommand{\lt}{<}
\newcommand{\gt}{>}

\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newcommand\demo{$\triangleleft$}

\newcommand{\qed}{\nobreak \ifvmode \relax \else
      \ifdim\lastskip<1.5em \hskip-\lastskip
      \hskip1.5em plus0em minus0.5em \fi \nobreak
      \vrule height0.75em width0.5em depth0.25em\fi}
%opening
\title{Fundamental Concepts: Transformation, Rejection, and Reweighting}
\author{Peadar Coyle}

\begin{document}

\maketitle

\begin{abstract}

\end{abstract}

\section{Fundamental concepts: Sampling}
A fundamental concept in using Monte Carlo methods is sampling.
\subsection{Transformation methods}
One of the simplest methods of generating random samples from a distribution with cumulative distribution function 
(c.d.f.) F(x) = $\mathbb{P}(X \leq x)$ is based on the inverse of the c.d.f.
   The c.d.f. is an increasing function, however it is not necessarily continuous. Thus
   we define the \textit{generalised inverse} $F^{-}(u)\;=\; \inf\left\lbrace x\;:\;F(x) \geq u\right\rbrace$

   \begin{theorem}[Inversion Method] Let U $\simeq$ $\bigcup[0,1]$ and F be a c.d.f. Then $F^{-}(U)$ has the c.d.f. F.
    \begin{proof}
     It is easy to see (by drawing a diagram say) that $F^{-}(u) \;\leq\;x$ is equivalent to $u \; \leq\; F(x).$
     Thus for $U\;\simeq\;\bigcup[0,1]$
   \newline
   \begin{displaymath}
    \mathbb{P}(F^{-}(U) \leq x)\;=\;\mathbb{P}(U\leq F(x))\;=\;F(x),
   \end{displaymath}
thus F is the c.d.f. of X = $F^{-}(U).$

    \end{proof}

   \end{theorem}
Let us consider a classical example.
\begin{example}[Exponential Distribution].\label{3.1}
The exponential distribution with $\lambda$ > 0 has the c.d.f. $F_{\lambda}(x)= 1 - \exp(-\lambda x)$ for $x$ $\geq$ 0.
Thus $F^{-}_{\lambda}(x)$ = $F^{-1}_{\lambda}(u)$ = $-log(1 - u)/ \lambda$. Thus we can generate random samples from 
$Expo(\lambda)$ by applying the transformation $-log(1-U)/ \lambda$ to a uniform $\bigcup[0,1]$ random variable U. As
U and 1-U, of course have the same distribution we can use $-log(U)/ \lambda$ as well.       \quad\quad$\triangleleft$
 
\end{example}
  The Inversion Method is a very efficient tool for generating random numbers. However very few distributions 
  possess a c.d.f. whose (generalised) inverse can be evaluated efficiently. Take the example of the Gaussian distribution
  , whose c.d.f. is not even available in closed form. 
     Nevertheless there are other examples of transformations. An example is the Box-Muller method for
     generating Gaussian random variables.
  \begin{example}[Box-Muller Method for Sampling from Gaussians]
   When sampling from the normal distribution, one faces the problem that neither the c.d.f. $\phi(\cdot),$ nor
   its inverse has a closed-form expression. Thus we cannnot use the inversion method. 
 It turns out however, that if we consider a pair $X_1,X_2$ $\stackrel{i.i.d.}\sim$ $N(0,1)$, as a point $(X_1,X_2)$
  in the plane, then its polar coordinates $(R,\theta)$ are again independent and have distributions we can easily
  sample from: $\bigcup[0,2\pi]$ and $R^{2} \sim Expo(1/2)$. Then the joint density of $(\theta, r^{2})$
  is 
 \begin{equation*}
  f_{(\theta,r^{2})}(\theta, r^{2}) = \frac{1}{2 \pi} 1_{[0,2\pi]}(\theta) \cdot 
  \frac{1}{2} \exp \left(- \frac{1}{2}r^2\right) \cdot 1_{[0,2\pi]}(\theta)
 \end{equation*}
To obtain the probability density function of 
\begin{equation*}
 X_1 = \sqrt{R^{2}}\cdot \cos(\theta) \;\;\;\;\; X_2 = \sqrt{R^{2}}\cdot \sin(\theta)
\end{equation*}
we need to use the transformaiton of densities formula.
\begin{equation*}
 f_{(X_1,X_2)}(x_1,x_2)=f_{(\theta,r^{2})}(\theta(x_1,x_2),r^{2}(x_1,x_2))\cdot \begin{vmatrix}
\dfrac{\partial x_1}{\partial \theta} & \dfrac{\partial x_1}{\partial r^{2}} \\[3pt]
\dfrac{\partial x_2}{\partial \theta} & \dfrac{\partial x_2}{\partial r^{2}} \\[3pt]

\end{vmatrix}^{-1} = \frac{1}{4\pi}\exp\left(-\frac{1}{2}(x_1^{2} + x_2^{2})^{2}\right)\cdot 2
\end{equation*}
\begin{flushright}
\begin{displaymath}
 = \left(\frac{1}{\sqrt{2\pi}}\exp \left(-\frac{1}{2}x^{2}_{1}\right)\right)\cdot \left(\frac{1}{\sqrt{2\pi}}\exp \left(-\frac{1}{2}x^{2}_{2}\right)\right)
\end{displaymath}\end{flushright}
as 
\begin{displaymath}
 \begin{vmatrix}
\dfrac{\partial x_1}{\partial \theta} & \dfrac{\partial x_1}{\partial r^{2}} \\[3pt]
\dfrac{\partial x_2}{\partial \theta} & \dfrac{\partial x_2}{\partial r^{2}} \\[3pt]

\end{vmatrix} = \begin{vmatrix}
-r\sin(\theta) & \dfrac{\cos(\theta)}{2r} \\[3pt]
r\cos(\theta) & \dfrac{\sin (\theta)}{2r} \\[3pt]

\end{vmatrix} =  \begin{vmatrix}
-\dfrac{r\sin(\theta)}{2r} & - \dfrac{r\cos(\theta)^{2}}{2r}

\end{vmatrix}
= \frac{1}{2}
\end{displaymath}


Thus $X_1,X_2$ $\sim$ $N(0,1).$ As their joint density factorises, $X_1$ and $X_2$ are independent, as required. 
   Thus we only need to generate $\theta$ $\sim$ $\bigcup[0,2\pi]$, and $R^{2} \sim Expo(1/2)$.
   Using $U_1$,$U_2$ $\stackrel{i.i.d.}{\sim} \bigcup[0,1]$ and example \ref{3.1} we can generate R = $\sqrt{R^{2}}$
   and $\theta$ by 
   \begin{equation*}
    R = \sqrt{-2\log(U_1)},\;\;\;\;\; \theta = 2\pi U_2,
   \end{equation*}
and thus
\begin{equation*}
 X_1 = \sqrt{-2 \log (U_1)}\cdot \cos(2\pi U_2), \;\; X_2 = \sqrt{-2\log(U_1}\cdot \sin(2\pi U_2)
\end{equation*}
are two independent realisations from a $N(0,1)$ distribution  \quad\quad$\triangleleft$              \end{example}

 The idea of transformation methods like the Inversion Method was to generate random samples
 from a distribution other than the target distribution and to transform them such that they come
 from the desired target distribution. In many situatioons, we cannot find such a transformation in closed form.
 In these cases we have to find other ways of correcting for the fact that we sample from the 'wrong' distribution.
 The next two sections present two such ideas: rejection sampling and importance sampling.
 \subsection{Rejection sampling}
 The basic idea of rejection sampling is to sample from an \textit{instrumental distribution} and reject samples
 that are 'unlikely' under the target distribution.
    Assume that we want to sample from a target distribution whose density f is known to us. The simple idea
    underlying rejection sampling (and other Monte Carlo algorithms) is the rather trivial identity
    \begin{equation*}
     f(x) = \int^{f(x)}_{0} 1 du = \int^1_0 1 _{0\lt u \lt f(x)} du
    \end{equation*}
Thus f(x) can be interpreted as the marginal density of a uniform distribution on the area under the density f(x)
\begin{center}
\begin{equation*}
 \left\lbrace (x,u):0\leq u\leq f(x)\right\rbrace
\end{equation*}\end{center}
     Consider for instance the beta distribution
     \begin{example}[Sampling from a Beta distribution]\label{3.3}
     The $\mathtt{Beta(a,b)}$ distribution (a,b $\geq$ 0) has the density
     \begin{equation*}
      f(x) = \dfrac{\Gamma(a + b)}{\Gamma(a)\Gamma(b)}x^{a-1}(1-x)^{b-1},\quad for\;0\lt x \lt 1,
     \end{equation*}
where $\Gamma(z) = \int^{+\infty}_{0}t^{a-1}\exp(-t)dt$ is the Gamma function. For a,b $\gt$ 1 the $\mathtt{Beta(a,b)}$ density
is unimodal \footnote{Think of what happens in the discrete case, when we count the number of items in a distribution,
the most often occuring is the mode, which is clearly the maximum of the distribution, in the continuous case} with
mode (a-1)/(a+b-2). It attains its maximum at 1680/729 $\approx$ 2.305 at x = 1/3.
Using the above identity we can draw from $\mathtt{Beta(a,b)}$ by drawing from a uniform distribution on the area 
under the density (the area under the curve - if the distribution is plotted). We sample from the area under the 
density, by sampling from the rectangle and keeping only the samples that fall in the area under the curve. 
Mathematically speaking, we sample independently $X \sim \bigcup[0,1]$ and $U \sim \bigcup[0,2.4]$.
We keep the pair (X,U) if $X \lt f(X)$, otherwise we reject it.
The conditional probability that a pair (X,U) is kept if X = x is 
\begin{equation*}
 \mathbb{P}(U \lt f(X)|X = x) = \mathbb{P}(U \lt f(x)) = f(x)/2.4
\end{equation*}
As X and U are drawn independently we can rewrite our algorithm as: Draw X from $\bigcup[0,1]$ and accept X with
probability f(X)/2.4 otherwise reject X.
 
      \quad\quad\quad$\triangleleft$
     \end{example}
  The method proposed in \ref{3.3} is based on bounding the density of the Beta distribution by a box. Whilst
  this is a powerful idea, it cannot be directly applied to other distributions, as the density might be unbounded
  or have infinite support. However we might be able to bound the density of f(x) by M$\cdot$g(x), where g(x) is a 
  density we can easily sample from. 
\begin{algorithm}[Rejection sampling].
Given two densities f,g with f(x) $\lt$ M$\cdot$g(x) for all x, we can generate a sample from f as follows:
\begin{enumerate}
 \item Draw X $\sim$ g
 \item Accept X as a sample from f with probability
   \begin{equation*}
    \frac{f(X)}{M\cdot g(X)},
   \end{equation*}

\end{enumerate}
otherwise return to step 1.
\begin{proof}
 We have
 \begin{equation}
  \mathbb{P}(X \in \xi and\;is\;accepted) = \int_{\xi} g(x) \dfrac{f(x)}{M \cdot g(x)} dx = \dfrac{\int_{\xi}f(x) dx}{M}
 \end{equation}
 and thus\footnote{We denote by S the set of all possible values X can take, i.e. $\int_{S} f(x)dx = 1$}
 \begin{equation}
  \mathbb{P}(X\;is\;accepted)= \mathbb{P}( X\;\in\;S\;and\;is\;accepted) = \dfrac{1}{M}
 \end{equation}yielding \begin{equation}
\mathbb{P}(x \in \xi| X\;is\;accepted) = \dfrac{\mathbb{P}(X \in \xi\;and\;is\;accepted)}{\mathbb{P}(X\;is\;accepted}
  = \dfrac{\int_{\xi} f(x) dx/M}{1/M} = \int_{\xi} f(x)dx
 \end{equation}
 \rmfamily{Thus the density of the values accepted by the algorithm is f}($\cdot$)
 \end{proof}

 
\end{algorithm}

 \subsection{Importance sampling}
In rejection sampling we have compensated for the fact that we sampled from the instrumental distribution $g(x)$
instead of f(x) by rejecting some of the values proposed by g(x). Importance sampling is based on the idea of using 
weights to correct for the fact that we sample from the instrumental distribution $g(x)$ instead of the target
distribution $f(x)$.
   Importance sampling is based on the identity 
   \begin{equation}\label{3.4}
    \mathbb{P}(X\in A) = \int_A f(x)dx = \int_A g(x)\dfrac{f(x)}{g(x)}dx = \int_A g(x) w(x)dx
   \end{equation}
for all $g(\cdot)$, such that $g(x)$ $\gt$ 0 for (almost) all x with f(x) $\gt$. We can generalise this identity
by considering the expectation $\mathbb{E}_{f}(h(X))$ of a measurable function h:
\begin{equation}\label{3.5}
 \mathbb{E}_{f}(h(X)) = \int_S f(x)h(x)dx = \int_S g(x) \dfrac{f(x)}{g(x)} h(x)dx = \int_S g(x)w(x)h(x)dx = 
 \mathbb{E}_{g}(w(X)\cdot h(X)),
\end{equation}
if g(x) $\gt$ 0 for (almost) all x with f(x).h(x) $\neq$ 0.
   Assume we have have a sample $X_1,\cdots, X_n \sim g.$ Then, provided $\mathbb{E}_{g}|w(X)\cdot h(X)|$ exists,
   \begin{equation*}
   \frac{1}{n} \sum_{i=1}^{n} w(X_i)h(X_i) \stackrel{a.s.}{\stackrel{n \rightarrow \infty}{\rightarrow}} \mathbb{E}_{g}(w(X) \cdot h(X))
   \end{equation*}
(by the Law of Large Numbers) and thus by (\ref{3.5})
\begin{equation*}
\frac{1}{n} \sum_{i=1}^{n} w(X_i)h(X_i) \stackrel{a.s.}{\stackrel{n \rightarrow \infty}{\rightarrow}} \mathbb{E}_{f}(h(X))
\end{equation*}
In other words, we can estimate $\mu = \mathbb{E}_{g}(h(X))$ by
\begin{equation*}
 \tilde{\mu} =  \frac{1}{n} \sum_{i=1}^{n} w(X_i)h(X_i) 
\end{equation*}
Note that whilst $\mathbb{E}_{g}(w(X)) = \int_S \dfrac{f(x)}{g(x)}g(x)dx = \int_S f(x) = 1 $, the weights $w_1(X),\cdots,w_n(X)$
do not necessarily sum up the n, so one might want to consider the \textit{self-normalised} version 
\begin{equation*}\hat{\mu} =  \frac{1}{\sum_{i=1}^{n}w(X_i} \sum_{i=1}^{n} w(X_i)h(X_i) 
\end{equation*}
  \begin{algorithm}[Importance Sampling]
  \begin{enumerate}
   \item For i = 1,$\cdots$,n:
   \begin{itemize}
    \item Generate $X_i \sim g$.
    \item Set $w(X_i) = \dfrac{f(X_i)}{g(X_i}$
 
 \end{itemize}
\item Return either 
\begin{equation*}
 \tilde{\mu} =  \frac{1}{n} \sum_{i=1}^{n} w(X_i)h(X_i) 
\end{equation*}
or 
\begin{equation*}\hat{\mu} =  \frac{1}{\sum_{i=1}^{n}w(X_i} \sum_{i=1}^{n} w(X_i)h(X_i) 
\end{equation*}
  \end{enumerate}

   
  \end{algorithm}
There are theorems for bias and the variance of importance sampling.
\begin{theorem}[Bias and Variance of Importance Sampling]\label{Bias}
\begin{enumerate}
 \item $\mathbb{E}_{g}(\tilde{\mu}) = \mu$
 \item $Var_{g}(\tilde{\mu}) = \frac{Var_{g}(w(X)\cdot h(X)}{n}$

 \item $\mathbb{E}_{g}(\hat{\mu}) = \mu + \dfrac{\mu Var_{g}(w(X)) - Cov_{g}(w(X),w(X)\cdot h(X)}{n}$
 \end{enumerate}

\begin{proof}
 We shall only prove (1) and (2), the other two items are proved in Liu, 2001, p.35
 \footnote{Monte Carlo Methods in Scientific Computing}
 \begin{enumerate}
  \item $\mathbb{E}_{g}\left(  \frac{1}{n} \sum_{i=1}^{n} w(X_i)h(X_i) \right) = \frac{1}{n} \sum_{i=1}^{n} \mathbb{E}_{g}(w(X_i)h(X_i)) 
  = \mathbb{E}_{f}(h(X))$
  \item $Var_g\left(\frac{1}{n} \sum_{i=1}^{n} w(X_{i})h(X_i)\right) = \frac{1}{n^2} \sum_{i=1}^{n}Var_{g}(w(X_{i})h(X_i))
  = \dfrac{Var_{g}(w(X)h(X))}{n}$
 \end{enumerate}

\end{proof}
 
\end{theorem}
Note that the theorem implies that in contrast to $\tilde{\mu}$ the self-normalised estimator $\hat{\mu}$ is biased.
The self-normalised estimator $\hat{\mu}$ might however have a lower variance. In addition, it has another advantage: 
we only need to know the density up to a multiplicative constant, as it is often the case in hierarchical Bayesian
modelling. Assume f(x) = $C \cdot \pi(x)$, then 
\begin{equation*}
 \hat{\mu} = \dfrac{\sum_{i=1}^{n}w(X_i)h(X_i)}{\sum_{i=1}^{n}w(X_{i})} =\dfrac{\sum_{i=1}^{n} \frac{f(X_i)}{g(X_i)}h(X_i)}{\sum_{i=1}^{n}\frac{f(X_i)}{g(X_i)}}
= \dfrac{\sum_{i=1}^{n} \frac{C\cdot \pi(X_i)}{g(X_i)}h(X_i)}{\sum_{i=1}^{n}\frac{C\cdot \pi(X_i)}{g(X_i)}}
=  \dfrac{\sum_{i=1}^{n} \frac{\pi(X_i)}{g(X_i)}h(X_i)}{\sum_{i=1}^{n}\frac{\pi(X_i)}{g(X_i)}}
 \end{equation*}
i.e. the self-normalised estimator $\hat{\mu}$ does not depend on the normalisation constant C.
On the other hand, as we seen in the proof of the theorem \ref{Bias} that it is a lot harder to analyse the 
theoretical propoerties of the self-normalised estimator $\hat{\mu}.$
   Although the above equations (\ref{3.4}) and (\ref{3.5}) hold for every g with $supp(g)\supset supp(f.h)$ and the 
   importance sampling algorithm converges for a large choice of such g, one typically only considers choices
   of go that lead to \textit{finite variance estimators}.
   The following two conditions are each sufficient (albeit rather restrictive) for a finite variance of $\tilde{\mu}$
:
- f(x) $\lt M\cdot g(x)$ and $Var_{f}(h(X$
   \end{document}
