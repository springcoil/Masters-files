<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Peadar Coyle" />
  <title>Fundamental Concepts: Transformation, Rejection, and Reweighting</title>
  <script src="https://d3eoax9i5htok0.cloudfront.net/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<div id="header">
<h1 class="title">Fundamental Concepts: Transformation, Rejection, and Reweighting</h1>
<h2 class="author">Peadar Coyle</h2>
</div>
<h1 id="fundamental-concepts-sampling">Fundamental concepts: Sampling</h1>
<p>A fundamental concept in using Monte Carlo methods is sampling.</p>
<h2 id="transformation-methods">Transformation methods</h2>
<p>One of the simplest methods of generating random samples from a distribution with cumulative distribution function (c.d.f.) F(x) = \(\mathbb{P}(X \leq x)\) is based on the inverse of the c.d.f. The c.d.f. is an increasing function, however it is not necessarily continuous. Thus we define the <em>generalised inverse</em> \(F^{-}(u)\;=\; \inf\left\lbrace x\;:\;F(x) \geq u\right\rbrace\)</p>
<p>[Inversion Method] Let U \(\simeq\) \(\bigcup[0,1]\) and F be a c.d.f. Then \(F^{-}(U)\) has the c.d.f. F.</p>
<p>It is easy to see (by drawing a diagram say) that \(F^{-}(u) \;\leq\;x\) is equivalent to \(u \; \leq\; F(x).\) Thus for \(U\;\simeq\;\bigcup[0,1]\)</p>
<p>\[\mathbb{P}(F^{-}(U) \leq x)\;=\;\mathbb{P}(U\leq F(x))\;=\;F(x),\]</p>
<p>thus F is the c.d.f. of X = \(F^{-}(U).\)</p>
<p>Let us consider a classical example.</p>
<p>[Exponential Distribution].[3.1] The exponential distribution with \(\lambda\) &gt; 0 has the c.d.f. \(F_{\lambda}(x)= 1 - \exp(-\lambda x)\) for \(x\) \(\geq\) 0. Thus \(F^{-}_{\lambda}(x)\) = \(F^{-1}_{\lambda}(u)\) = \(-log(1 - u)/ \lambda\). Thus we can generate random samples from \(Expo(\lambda)\) by applying the transformation \(-log(1-U)/ \lambda\) to a uniform \(\bigcup[0,1]\) random variable U. As U and 1-U, of course have the same distribution we can use \(-log(U)/ \lambda\) as well. \(\triangleleft\)</p>
<p>The Inversion Method is a very efficient tool for generating random numbers. However very few distributions possess a c.d.f. whose (generalised) inverse can be evaluated efficiently. Take the example of the Gaussian distribution , whose c.d.f. is not even available in closed form. Nevertheless there are other examples of transformations. An example is the Box-Muller method for generating Gaussian random variables.</p>
<p>[Box-Muller Method for Sampling from Gaussians] When sampling from the normal distribution, one faces the problem that neither the c.d.f. \(\phi(\cdot),\) nor its inverse has a closed-form expression. Thus we cannnot use the inversion method. It turns out however, that if we consider a pair \(X_1,X_2\) \(\stackrel{i.i.d.}\sim\) \(N(0,1)\), as a point \((X_1,X_2)\) in the plane, then its polar coordinates \((R,\theta)\) are again independent and have distributions we can easily sample from: \(\bigcup[0,2\pi]\) and \(R^{2} \sim Expo(1/2)\). Then the joint density of \((\theta, r^{2})\) is</p>
<p>\[f_{(\theta,r^{2})}(\theta, r^{2}) = \frac{1}{2 \pi} 1_{[0,2\pi]}(\theta) \cdot 
  \frac{1}{2} \exp \left(- \frac{1}{2}r^2\right) \cdot 1_{[0,2\pi]}(\theta)\]</p>
<p>To obtain the probability density function of</p>
<p>\[X_1 = \sqrt{R^{2}}\cdot \cos(\theta) \;\;\;\;\; X_2 = \sqrt{R^{2}}\cdot \sin(\theta)\]</p>
<p>we need to use the transformaiton of densities formula.</p>
<p>\[f_{(X_1,X_2)}(x_1,x_2)=f_{(\theta,r^{2})}(\theta(x_1,x_2),r^{2}(x_1,x_2))\cdot \begin{vmatrix}
\dfrac{\partial x_1}{\partial \theta} &amp; \dfrac{\partial x_1}{\partial r^{2}} \\[3pt]
\dfrac{\partial x_2}{\partial \theta} &amp; \dfrac{\partial x_2}{\partial r^{2}} \\[3pt]

\end{vmatrix}^{-1} = \frac{1}{4\pi}\exp\left(-\frac{1}{2}(x_1^{2} + x_2^{2})^{2}\right)\cdot 2\]</p>
<p>\[= \left(\frac{1}{\sqrt{2\pi}}\exp \left(-\frac{1}{2}x^{2}_{1}\right)\right)\cdot \left(\frac{1}{\sqrt{2\pi}}\exp \left(-\frac{1}{2}x^{2}_{2}\right)\right)\]</p>
<p>as</p>
<p>\[\begin{vmatrix}
\dfrac{\partial x_1}{\partial \theta} &amp; \dfrac{\partial x_1}{\partial r^{2}} \\[3pt]
\dfrac{\partial x_2}{\partial \theta} &amp; \dfrac{\partial x_2}{\partial r^{2}} \\[3pt]

\end{vmatrix} = \begin{vmatrix}
-r\sin(\theta) &amp; \dfrac{\cos(\theta)}{2r} \\[3pt]
r\cos(\theta) &amp; \dfrac{\sin (\theta)}{2r} \\[3pt]

\end{vmatrix} =  \begin{vmatrix}
-\dfrac{r\sin(\theta)}{2r} &amp; - \dfrac{r\cos(\theta)^{2}}{2r}

\end{vmatrix}
= \frac{1}{2}\]</p>
<p>Thus \(X_1,X_2\) \(\sim\) \(N(0,1).\) As their joint density factorises, \(X_1\) and \(X_2\) are independent, as required. Thus we only need to generate \(\theta\) \(\sim\) \(\bigcup[0,2\pi]\), and \(R^{2} \sim Expo(1/2)\). Using \(U_1\),\(U_2\) \(\stackrel{i.i.d.}{\sim} \bigcup[0,1]\) and example [3.1] we can generate R = \(\sqrt{R^{2}}\) and \(\theta\) by</p>
<p>\[R = \sqrt{-2\log(U_1)},\;\;\;\;\; \theta = 2\pi U_2,\]</p>
<p>and thus</p>
<p>\[X_1 = \sqrt{-2 \log (U_1)}\cdot \cos(2\pi U_2), \;\; X_2 = \sqrt{-2\log(U_1}\cdot \sin(2\pi U_2)\]</p>
<p>are two independent realisations from a \(N(0,1)\) distribution \(\triangleleft\)</p>
<p>The idea of transformation methods like the Inversion Method was to generate random samples from a distribution other than the target distribution and to transform them such that they come from the desired target distribution. In many situatioons, we cannot find such a transformation in closed form. In these cases we have to find other ways of correcting for the fact that we sample from the ’wrong’ distribution. The next two sections present two such ideas: rejection sampling and importance sampling.</p>
<h2 id="rejection-sampling">Rejection sampling</h2>
<p>The basic idea of rejection sampling is to sample from an <em>instrumental distribution</em> and reject samples that are ’unlikely’ under the target distribution. Assume that we want to sample from a target distribution whose density f is known to us. The simple idea underlying rejection sampling (and other Monte Carlo algorithms) is the rather trivial identity</p>
<p>\[f(x) = \int^{f(x)}_{0} 1 du = \int^1_0 1 _{0&lt; u &lt; f(x)} du\]</p>
<p>Thus f(x) can be interpreted as the marginal density of a uniform distribution on the area under the density f(x)</p>
<p>\[\left\lbrace (x,u):0\leq u\leq f(x)\right\rbrace\]</p>
<p>Consider for instance the beta distribution</p>
<p>[Sampling from a Beta distribution][3.3] The \(\mathtt{Beta(a,b)}\) distribution (a,b \(\geq\) 0) has the density</p>
<p>\[f(x) = \dfrac{\Gamma(a + b)}{\Gamma(a)\Gamma(b)}x^{a-1}(1-x)^{b-1},\quad for\;0&lt; x &lt; 1,\]</p>
<p>where \(\Gamma(z) = \int^{+\infty}_{0}t^{a-1}\exp(-t)dt\) is the Gamma function. For a,b \(&gt;\) 1 the \(\mathtt{Beta(a,b)}\) density is unimodal <sup><a href="#fn1" class="footnoteRef" id="fnref1">1</a></sup> with mode (a-1)/(a+b-2). It attains its maximum at 1680/729 \(\approx\) 2.305 at x = 1/3. Using the above identity we can draw from \(\mathtt{Beta(a,b)}\) by drawing from a uniform distribution on the area under the density (the area under the curve - if the distribution is plotted). We sample from the area under the density, by sampling from the rectangle and keeping only the samples that fall in the area under the curve. Mathematically speaking, we sample independently \(X \sim \bigcup[0,1]\) and \(U \sim \bigcup[0,2.4]\). We keep the pair (X,U) if \(X &lt; f(X)\), otherwise we reject it. The conditional probability that a pair (X,U) is kept if X = x is</p>
<p>\[\mathbb{P}(U &lt; f(X)|X = x) = \mathbb{P}(U &lt; f(x)) = f(x)/2.4\]</p>
<p>As X and U are drawn independently we can rewrite our algorithm as: Draw X from \(\bigcup[0,1]\) and accept X with probability f(X)/2.4 otherwise reject X.</p>
<p>\(\triangleleft\)</p>
<p>The method proposed in [3.3] is based on bounding the density of the Beta distribution by a box. Whilst this is a powerful idea, it cannot be directly applied to other distributions, as the density might be unbounded or have infinite support. However we might be able to bound the density of f(x) by M\(\cdot\)g(x), where g(x) is a density we can easily sample from.</p>
<p>[Rejection sampling]. Given two densities f,g with f(x) \(&lt;\) M\(\cdot\)g(x) for all x, we can generate a sample from f as follows:</p>
<ol>
<li><p>Draw X \(\sim\) g</p></li>
<li><p>Accept X as a sample from f with probability</p>
<p>\[\frac{f(X)}{M\cdot g(X)},\]</p></li>
</ol>
<p>otherwise return to step 1.</p>
<p>We have</p>
<p>\[\mathbb{P}(X \in \xi and\;is\;accepted) = \int_{\xi} g(x) \dfrac{f(x)}{M \cdot g(x)} dx = \dfrac{\int_{\xi}f(x) dx}{M}\]</p>
<p>and thus<sup><a href="#fn2" class="footnoteRef" id="fnref2">2</a></sup></p>
<p>\[\mathbb{P}(X\;is\;accepted)= \mathbb{P}( X\;\in\;S\;and\;is\;accepted) = \dfrac{1}{M}\]</p>
<p>yielding</p>
<p>\[\mathbb{P}(x \in \xi| X\;is\;accepted) = \dfrac{\mathbb{P}(X \in \xi\;and\;is\;accepted)}{\mathbb{P}(X\;is\;accepted}
  = \dfrac{\int_{\xi} f(x) dx/M}{1/M} = \int_{\xi} f(x)dx\]</p>
<p>Thus the density of the values accepted by the algorithm is f(\(\cdot\))</p>
<h2 id="importance-sampling">Importance sampling</h2>
<p>In rejection sampling we have compensated for the fact that we sampled from the instrumental distribution \(g(x)\) instead of f(x) by rejecting some of the values proposed by g(x). Importance sampling is based on the idea of using weights to correct for the fact that we sample from the instrumental distribution \(g(x)\) instead of the target distribution \(f(x)\). Importance sampling is based on the identity</p>
<p>\[\label{3.4}
    \mathbb{P}(X\in A) = \int_A f(x)dx = \int_A g(x)\dfrac{f(x)}{g(x)}dx = \int_A g(x) w(x)dx\]</p>
<p>for all \(g(\cdot)\), such that \(g(x)\) \(&gt;\) 0 for (almost) all x with f(x) \(&gt;\). We can generalise this identity by considering the expectation \(\mathbb{E}_{f}(h(X))\) of a measurable function h:</p>
<p>\[\label{3.5}
 \mathbb{E}_{f}(h(X)) = \int_S f(x)h(x)dx = \int_S g(x) \dfrac{f(x)}{g(x)} h(x)dx = \int_S g(x)w(x)h(x)dx = 
 \mathbb{E}_{g}(w(X)\cdot h(X)),\]</p>
<p>if g(x) \(&gt;\) 0 for (almost) all x with f(x).h(x) \(\neq\) 0. Assume we have have a sample \(X_1,\cdots, X_n \sim g.\) Then, provided \(\mathbb{E}_{g}|w(X)\cdot h(X)|\) exists,</p>
<p>\[\frac{1}{n} \sum_{i=1}^{n} w(X_i)h(X_i) \stackrel{a.s.}{\stackrel{n \rightarrow \infty}{\rightarrow}} \mathbb{E}_{g}(w(X) \cdot h(X))\]</p>
<p>(by the Law of Large Numbers) and thus by ([3.5])</p>
<p>\[\frac{1}{n} \sum_{i=1}^{n} w(X_i)h(X_i) \stackrel{a.s.}{\stackrel{n \rightarrow \infty}{\rightarrow}} \mathbb{E}_{f}(h(X))\]</p>
<p>In other words, we can estimate \(\mu = \mathbb{E}_{g}(h(X))\) by</p>
<p>\[\tilde{\mu} =  \frac{1}{n} \sum_{i=1}^{n} w(X_i)h(X_i)\]</p>
<p>Note that whilst \(\mathbb{E}_{g}(w(X)) = \int_S \dfrac{f(x)}{g(x)}g(x)dx = \int_S f(x) = 1 \), the weights \(w_1(X),\cdots,w_n(X)\) do not necessarily sum up the n, so one might want to consider the <em>self-normalised</em> version</p>
<p>\[\hat{\mu} =  \frac{1}{\sum_{i=1}^{n}w(X_i} \sum_{i=1}^{n} w(X_i)h(X_i)\]</p>
<p>[Importance Sampling]</p>
<ol>
<li><p>For i = 1,\(\cdots\),n:</p>
<ul>
<li><p>Generate \(X_i \sim g\).</p></li>
<li><p>Set \(w(X_i) = \dfrac{f(X_i)}{g(X_i}\)</p></li>
</ul></li>
<li><p>Return either</p>
<p>\[\tilde{\mu} =  \frac{1}{n} \sum_{i=1}^{n} w(X_i)h(X_i)\]</p>
<p>or</p>
<p>\[\hat{\mu} =  \frac{1}{\sum_{i=1}^{n}w(X_i} \sum_{i=1}^{n} w(X_i)h(X_i)\]</p></li>
</ol>
<p>There are theorems for bias and the variance of importance sampling.</p>
<p>[Bias and Variance of Importance Sampling][Bias]</p>
<ol>
<li><p>\(\mathbb{E}_{g}(\tilde{\mu}) = \mu\)</p></li>
<li><p>\(Var_{g}(\tilde{\mu}) = \frac{Var_{g}(w(X)\cdot h(X)}{n}\)</p></li>
<li><p>\(\mathbb{E}_{g}(\hat{\mu}) = \mu + \dfrac{\mu Var_{g}(w(X)) - Cov_{g}(w(X),w(X)\cdot h(X)}{n}\)</p></li>
</ol>
<p>We shall only prove (1) and (2), the other two items are proved in Liu, 2001, p.35 <sup><a href="#fn3" class="footnoteRef" id="fnref3">3</a></sup></p>
<ol>
<li><p>\(\mathbb{E}_{g}\left(  \frac{1}{n} \sum_{i=1}^{n} w(X_i)h(X_i) \right) = \frac{1}{n} \sum_{i=1}^{n} \mathbb{E}_{g}(w(X_i)h(X_i)) 
  = \mathbb{E}_{f}(h(X))\)</p></li>
<li><p>\(Var_g\left(\frac{1}{n} \sum_{i=1}^{n} w(X_{i})h(X_i)\right) = \frac{1}{n^2} \sum_{i=1}^{n}Var_{g}(w(X_{i})h(X_i))
  = \dfrac{Var_{g}(w(X)h(X))}{n}\)</p></li>
</ol>
<p>Note that the theorem implies that in contrast to \(\tilde{\mu}\) the self-normalised estimator \(\hat{\mu}\) is biased. The self-normalised estimator \(\hat{\mu}\) might however have a lower variance. In addition, it has another advantage: we only need to know the density up to a multiplicative constant, as it is often the case in hierarchical Bayesian modelling. Assume f(x) = \(C \cdot \pi(x)\), then</p>
<p>\[\hat{\mu} = \dfrac{\sum_{i=1}^{n}w(X_i)h(X_i)}{\sum_{i=1}^{n}w(X_{i})} =\dfrac{\sum_{i=1}^{n} \frac{f(X_i)}{g(X_i)}h(X_i)}{\sum_{i=1}^{n}\frac{f(X_i)}{g(X_i)}}
= \dfrac{\sum_{i=1}^{n} \frac{C\cdot \pi(X_i)}{g(X_i)}h(X_i)}{\sum_{i=1}^{n}\frac{C\cdot \pi(X_i)}{g(X_i)}}
=  \dfrac{\sum_{i=1}^{n} \frac{\pi(X_i)}{g(X_i)}h(X_i)}{\sum_{i=1}^{n}\frac{\pi(X_i)}{g(X_i)}}\]</p>
<p>i.e. the self-normalised estimator \(\hat{\mu}\) does not depend on the normalisation constant C. On the other hand, as we seen in the proof of the theorem [Bias] that it is a lot harder to analyse the theoretical propoerties of the self-normalised estimator \(\hat{\mu}.\) Although the above equations ([3.4]) and ([3.5]) hold for every g with \(supp(g)\supset supp(f.h)\) and the importance sampling algorithm converges for a large choice of such g, one typically only considers choices of go that lead to <em>finite variance estimators</em>. The following two conditions are each sufficient (albeit rather restrictive) for a finite variance of \(\tilde{\mu}\) : - f(x) \(&lt; M\cdot g(x)\) and \(Var_{f}(h(X)) \; &lt;; + \infty.\) - S is compact, f is bounded above on S, and g is bounded below on S. Note that under the first condition rejection sampling can also be used to sample from f.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Think of what happens in the discrete case, when we count the number of items in a distribution, the most often occuring is the mode, which is clearly the maximum of the distribution, in the continuous case<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>We denote by S the set of all possible values X can take, i.e. \(\int_{S} f(x)dx = 1\)<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>Monte Carlo Methods in Scientific Computing<a href="#fnref3">↩</a></p></li>
</ol>
</div>
</body>
</html>
